
强化学习思路：

    使用强化学习（RL）的目标不是让LLM学习如何说话，而是让系统学会如何更智能地引导对话流程。它解决的是“在几十个可能的话题（维度）中，我下一步应该问哪个最合适？”的问题。
    核心目标: 实现对话流程的高度个性化和自适应，让Agent像一个经验丰富的治疗师一样，能敏锐地抓住重点，动态调整谈话方向，而不是死板地按1、2、3...的顺序提问。
    技术组件详解:
    智能体 (Agent): 系统的 CaiTI Questioner 模块。它的核心任务就是做决策：选择下一个要提问的问题维度。
    状态 (State, S): Agent所处的“环境”。在CaiTI中，状态被简化为当前刚刚结束提问的那个维度。例如，如果Agent刚问完“睡眠”问题，那么当前状态就是“睡眠”。系统共有39个状态（37个问题维度 + 1个开始状态 + 1个结束状态）。
    动作 (Action, A): Agent可以执行的操作。在CaiTI中，动作就是从所有37个维度中选择一个作为下一个提问的主题。
    奖励 (Reward, R): Agent执行一个动作后，环境给出的“反馈分数”。这是RL的核心。
    奖励来源: 来源于 Response Analyzer 模块对用户回答的评分。
    奖励机制:
    如果用户对问题的回答被评为 Score=2（表示存在明显问题，需要关注），Agent会获得一个高额正奖励。
    如果评分为 Score=1（表示存在一些问题），则获得一个中等正奖励。
    如果评分为 Score=0（表示情况良好），则获得一个零奖励或微小的负奖励。
    目的: 这个机制激励Agent去主动发现和探讨用户存在问题的领域，因为这样做能获得更高的累计奖励。
    策略 (Policy) - Q-learning & Q-Table:
    Q-Table: Agent学习的核心，可以想象成一张巨大的“决策表”。表格的行是所有可能的状态（State），列是所有可能的动作（Action）。表格中的每个单元格 Q(S, A) 存储一个数值，代表“在状态S下，执行动作A，预期未来能获得的总奖励是多少”。
    Q-learning: 学习和更新这张Q-Table的算法。每次交互后，Agent都会根据获得的奖励（Reward）来微调Q-Table中对应的值，使得这张“决策表”越来越准确。
    Epsilon-Greedy策略: 为了平衡“利用”和“探索”，Agent在做决策时：
    有 ε 的概率（比如10%）随机选择一个动作（探索新路径）。
    有 1-ε 的概率（比如90%）选择当前Q-Table中预估奖励最高的那个动作（利用已知最佳路径）。
    工作流程示例:
    启动: 咨询开始。CaiTI Questioner 处于“Start”状态。它的Q-Table已经由人类治疗师预设了初始值（即专家经验）。
    第一轮:
    动作选择: Agent查看Q-Table中“Start”状态对应的所有动作，发现“提问‘情绪’维度”的Q值最高。它决定提问关于情绪的问题。
    用户回应: 用户回答：“最近情绪很低落。”
    分析与奖励: Response Analyzer 将此回答评为 (Dimension: 情绪, Score: 2)。Agent因此获得了高额奖励。
    学习: Agent使用Q-learning算法，更新Q-Table中 Q("Start", "提问情绪") 的值，使其变得更高。它“学到”了：从情绪开始提问是一个好策略。
    状态转换: Agent的当前状态变为“情绪”。
    第二轮:
    动作选择: Agent查看Q-Table中“情绪”状态对应的所有动作。假设根据专家经验，“提问‘饮食’”的Q值较高。它选择提问饮食问题。
    用户回应: 用户回答：“我饮食很规律。”
    分析与奖励: Response Analyzer 评分为 (Dimension: 饮食, Score: 0)。Agent获得了零奖励。
    学习: Agent更新Q-Table中 Q("情绪", "提问饮食") 的值，这个值可能会轻微下降。它“学到”了：在用户情绪低落时，紧接着问饮食可能不是最高效的路径。

督导师（Response Analyzer）评估模板（追加）:

- 输出格式（JSON）：
    {
        "score": <0|1|2>,
        "brief_reason": "简短说明给分依据（1-2句）",
        "suggestion": "对下一轮咨询师的具体建议（自然口语化，1句）"
    }

- 说明：
    - score=2 表示需要重点跟进（高风险或明显问题），score=1 表示需要关注，score=0 表示当前无明显问题。
    - suggestion 应当用来直接喂回咨询师的下轮 prompt，保证自然承接并避免一次性提出多个子问题。

例子:
    {
        "score": 2,
        "brief_reason": "用户报告持续性失眠与绝对化表述（‘完全没有意义’）",
        "suggestion": "请在下一轮以共情并引导用户回忆触发事件的方式，询问当时第一个出现的念头。"
    }
